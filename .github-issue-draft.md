# Build MCP Server for Semantic Documentation Search

## Source Reference
- **Article**: "How to expose any documentation to any LLM agent" by Eric J. Ma
- **Date**: October 19, 2025
- **URL**: https://ericmjl.github.io/blog/2025/10/19/how-to-expose-any-documentation-to-any-llm-agent/
- **Reference Implementation**: LlamaBot v0.13.10+ MCP server

## Problem Statement

**The Obsolescence Challenge**: LLMs become outdated as soon as their training sets are fixed, while documentation evolves constantly. AI agents remain frozen in time, working with knowledge that may be months or years out of date.

Current approaches have significant limitations:
- **Native tool docs**: Limited to how the tool fetches docs, can't access gated systems
- **Manual repo inclusion**: Doesn't scale, consumes context window tokens
- **Copy-paste/upload**: Maintenance overhead, no semantic search
- **Web search by agents**: Inefficient, requires multiple iterations

## Solution: MCP-Based Semantic Documentation Layer

Build an MCP server that provides semantic search over project documentation using a vector database. This enables AI agents to query documentation using natural language and receive structured, contextually relevant results.

### Core Architecture (from LlamaBot example)

```python
@mcp.tool()
def docs_search(query: str, limit: int = 5) -> dict:
    """Search through documentation and source code."""
    results = docstore.retrieve(query, n_results=limit)
    return {"query": query, "results": results}
```

**Technology Stack**:
- FastMCP: Protocol implementation
- LanceDB: Vector database with semantic search, hybrid search, and reranking
- CI/CD integration: Build database automatically with each update
- Package distribution: Ship pre-built database for zero-setup experience

## Implementation Plan

### Phase 1: Research & Architecture Design
- [ ] Study LlamaBot's MCP server implementation in detail
- [ ] Evaluate vector database options (LanceDB, Chroma, others)
- [ ] Design documentation extraction pipeline
- [ ] Define what knowledge to expose (see Content Scope below)
- [ ] Plan CI/CD integration strategy

### Phase 2: Build Documentation Database Pipeline
- [ ] Extract AI security documentation (MITRE ATLAS, OWASP, etc.)
- [ ] Parse agent and skill documentation from `.claude/`
- [ ] Extract MCP server documentation and examples
- [ ] Chunk content appropriately for semantic search
- [ ] Build LanceDB vector database with embeddings
- [ ] Test semantic search quality and relevance

### Phase 3: Create MCP Server
- [ ] Implement FastMCP server with `docs_search` tool
- [ ] Integrate with vector database
- [ ] Add metadata and relevance scoring
- [ ] Create launch command (e.g., `bun run mcp:docs-search`)
- [ ] Write MCP server configuration for `mcp-servers-config.json`
- [ ] Test with Claude Code and other MCP-compatible tools

### Phase 4: CI/CD Integration
- [ ] Create build script (`scripts/build_docs_db.sh` or similar)
- [ ] Integrate with existing CI/CD pipeline
- [ ] Package database with distribution
- [ ] Implement fallback for development environments
- [ ] Add versioning and update tracking

### Phase 5: Documentation & Optimization
- [ ] Document MCP server usage in CLAUDE.md
- [ ] Create examples of natural language queries
- [ ] Add troubleshooting guide
- [ ] Optimize chunking strategy based on usage patterns
- [ ] Implement feedback loop for search quality

## Content Scope

The MCP server should provide semantic search over:

1. **AI Security Knowledge Base**:
   - MITRE ATLAS reference (all 14 tactics, 56 techniques)
   - OWASP LLM Security (Top 10, defenses)
   - Compliance frameworks (NIST, ISO, EU AI Act)
   - Red team methodologies
   - Emerging threats
   - Threat intelligence and CVE tracking

2. **Agent Documentation**:
   - All agents in `.claude/agents/` (grimface, foundation, AI security agents)
   - Agent capabilities, use cases, invocation patterns
   - Best practices for agent usage

3. **Skill Documentation**:
   - All skills in `.claude/skills/`
   - Workflow phases, integration points
   - Examples and success criteria

4. **MCP Server Documentation**:
   - Available MCP servers and their capabilities
   - Configuration examples
   - Integration patterns

5. **Project Processes**:
   - Development workflows
   - Git conventions
   - Testing and deployment procedures
   - From CLAUDE.md and other process docs

## Example Queries (Natural Language)

```text
"How do I use the ATLAS Threat Modeling agent?"
"What are the latest LLM prompt injection defenses?"
"Show me examples of adversarial testing workflows"
"What MCP servers are available for GitLab integration?"
"How do I run the AI security assessment skill?"
"What are the MITRE ATLAS tactics for data poisoning?"
```

## Expected Benefits

1. **Dynamic Knowledge**: Agents always have access to current documentation
2. **Semantic Search**: Natural language queries instead of keyword matching
3. **Automatic Updates**: Database rebuilds with each CI/CD run
4. **Reduced Context Window**: Only relevant docs retrieved, not entire corpus
5. **Scalable**: Works across all MCP-compatible tools (Claude Code, Cursor, VSCode)
6. **Zero Setup**: Pre-built database ships with project

## Integration with Existing Infrastructure

This project leverages existing MCP infrastructure:
- MCP server configuration: `mcp-servers-config.json`
- MCP launch scripts: `package.json` scripts section
- MCP Inspector: `bun run dev:inspector` for debugging
- Existing MCP servers: Pattern to follow for implementation

## Success Criteria

- [ ] AI agents can query documentation using natural language
- [ ] Search returns relevant results with high precision
- [ ] Database updates automatically with documentation changes
- [ ] Works seamlessly with Claude Code and other MCP tools
- [ ] Zero-setup experience for users (pre-built database)
- [ ] Measurable reduction in context-switching for AI agents
- [ ] Improved accuracy of agent responses about project capabilities

## Open Questions

- [ ] Which vector database? (LanceDB per article, or alternatives?)
- [ ] Optimal chunking strategy for different doc types?
- [ ] How to handle versioning of documentation database?
- [ ] Should we support multiple databases (security vs general docs)?
- [ ] Integration with AI Security Update Skill for CVE tracking?
- [ ] Usage analytics to improve search quality over time?

## Future Enhancements

1. **Real-time Updates**: Rebuild database when documentation changes
2. **Cross-referencing**: Link related concepts across documents
3. **Usage Analytics**: Learn from query patterns to improve results
4. **Multi-modal**: Include code examples, diagrams, and screenshots
5. **Organizational Knowledge**: Extend to team processes, Slack archives, etc.

## Priority

**High** - This directly addresses a core limitation (documentation obsolescence) and leverages existing MCP infrastructure. The semantic search capability could significantly improve agent effectiveness, especially for AI security operations.

## Labels

`enhancement`, `mcp`, `documentation`, `ai-agents`, `semantic-search`, `vector-database`, `llamabot-inspired`

## References

- LlamaBot MCP server: https://github.com/ericmjl/llamabot (v0.13.10+)
- FastMCP: https://github.com/jlowin/fastmcp
- LanceDB: https://lancedb.com/
- Model Context Protocol: https://modelcontextprotocol.io/
